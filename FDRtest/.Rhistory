library(MSstats)
library("RDAVIDWebService")
if(!require(installr)) {
install.packages("installr");
require(installr)
} #load / install+load installr
updateR()
library(clusterProfiler)
pi
rnorm(1, 0 , 0.1)
#set.seed(1977)
setwd('E:/OneDrive/PhD/Project/Thesis/4_Pipeline/Pipeline/FDRtest')
#inFile <- "PXD004682_QPROTout_ProgNorm_2up_ThreshResults.csv"
outFile <- "QPROT_FDRtestOut.csv"
example_qprot_fdr <- read.table('PXD004682_QPROTout_QPROTNorm_2up', header = TRUE, sep = '\t')
#example_qprot_fdr <- example_qprot_fdr[order(example_qprot_fdr$Zstatistic), ]
#example_qprot_density <- read.table('example_qprot_density', header = TRUE, sep = '\t')
np <- nrow(example_qprot_fdr)
log_gaussian_pdf <- function(x, mu, sigmasq) {
out <- -0.5*(x-mu)^2/sigmasq - 0.5 * log(2 * pi * sigmasq)
return(out)
}
gaussian_pdf <- function(x, mu, sigmasq) {
return(exp(log_gaussian_pdf(x, mu, sigmasq)))
}
## initialising parameters
pi_0 <- 0.9
pi_1 <- 0.1
pi_true <- 0.1
nullmean <- rnorm(1, 0 , 0.1)
nullvar <- exp(rnorm(1, 0 , 0.1))
if(np > 10000) {
ntick <- 10000
} else {
ntick <- np
}
zstat <- example_qprot_fdr$Zstatistic
zstat_sort <- sort(zstat)
xmin <- zstat_sort[1] - 0.1
xmax <- zstat_sort[np] + 0.1
tickMarks <- NULL
f0 <- NULL
f1 <- NULL
f <- NULL
for (i in 1:ntick) {
tickMarks[i] <- xmin + i * (xmax - xmin) / ntick
}
for (i in 1:ntick) {
f0[i] <- 0
f0[i] <- f0[i] + gaussian_pdf(tickMarks[i], nullmean, nullvar)
f1[i] <- 0.5 * gaussian_pdf(tickMarks[i], -10, 1) +  0.5 * gaussian_pdf(tickMarks[i], 10, 1)
f[i] <- pi_true * f1[i] + (1.0-pi_true) * f0[i]
}
df <- NULL
df0 <- NULL
df1 <- NULL
for (i in 1:np) {
df[i] <- 0
df0[i] <- 0
df1[i] <- 0
}
## Overall nonparametric density for f
#getBandwidth()
##identifying the range to fit null component to the overall density
mm <- nullmean
vv <- nullvar
temp_sd <- sqrt(vv)
bandwidth <- 1.06 * temp_sd/(np^0.2)
denom <- 0
for (j in 1:np) {
denom <- denom + 1
}
for (i in 1:ntick) {
numer <- 0
for (j in 1:np) {
numer <- numer + gaussian_pdf((zstat[j] - tickMarks[i])/bandwidth, 0.0, 1.0)
}
f[i] <- numer / (denom * bandwidth)
}
## Check mean shift
max_y <- 0
tmp_mean <-NULL
for (i in 1:ntick) {
if(f[i] > max_y) {
max_y <- f[i]
tmp_mean <- tickMarks[i]
}
}
nullmean <- tmp_mean
## Fit gaussian unif
zz <- matrix(nrow = np, ncol = 2)
z_min <- zstat_sort[1] - 0.1
z_max <- zstat_sort[np] + 0.1
old_mean <- NULL
old_var <- NULL
for (k in 1:np) {
zz[k, 1] <- 0.1
zz[k, 2] <- 0.9
zz[k, 1] <- zz[k, 1] * exp(rnorm(1, 0 , 0.1))
zz[k, 2] <- zz[k, 2] * exp(rnorm(1, 0 , 0.1))
tmp_denom <- 0
tmp_denom <- tmp_denom + zz[k, 1]
tmp_denom <- tmp_denom + zz[k, 2]
zz[k, 1] <- zz[k, 1] / tmp_denom
zz[k, 2] <- zz[k, 2] / tmp_denom
}
counter <- 0
tol <- 1000
tmpl <- NULL
while(tol > 0.00001 && counter < 100) {
old_mean <- nullmean
old_var <- nullvar
tmp_num <- 0
tmp_denom <- 0
for (i in 1:np) {
tmp_num <- tmp_num + (zstat[i] * zz[i, 1])
}
for (i in 1:np) {
tmp_denom <- tmp_denom + (zz[i, 1])
}
# if(tmp_denom > 0) {
#     nullmean <- tmp_num / tmp_denom
# } else{
#     nullmean <- 0
# }
tmp_num <- 0
tmp_denom <- 0
for (i in 1:np) {
tmp_num <- tmp_num + ((zstat[i] - nullmean) ^ 2) * (zz[i, 1])
}
for (i in 1:np) {
tmp_denom <- tmp_denom + (zz[i, 1])
}
if(tmp_denom > 0) {
nullvar <- tmp_num / tmp_denom
} else{
nullvar <- 0
}
for (i in 1:np) {
tmpl[1] <- pi_0 * gaussian_pdf(zstat[i], nullmean, nullvar)
tmpl[2] <- pi_1 / (z_max - z_min)
tmp_denom <- 0
tmp_denom <- tmp_denom + tmpl[1]
tmp_denom <- tmp_denom + tmpl[2]
if (tmp_denom > 0) {
zz[i, 1] <- tmpl[1] / tmp_denom
zz[i, 2] <- tmpl[2] / tmp_denom
} else {
zz[i, 1] <- 0
zz[i, 2] <- 0
}
}
tmp <- 0
for (i in 1:np) {
tmp <- tmp + zz[i, 1]
}
tmp <- tmp / np
pi_0 <- tmp
tmp <- 0
for (i in 1:np) {
tmp <- tmp + zz[i, 2]
}
tmp <- tmp / np
pi_1 <- tmp
tol <- 0
tol <- tol + abs(old_mean - nullmean)
tol <- tol + abs(old_var - nullvar)
counter <- counter + 1
}
## Evaluate f0 and df0
for (i in 1:ntick) {
tmp <- 0
tmp <- tmp + (pi_0 * gaussian_pdf(tickMarks[i], nullmean, nullvar))
tmp <- tmp / (1.0 - pi_1)
f0[i] <- tmp
}
for(i in 1:np) {
tmp <- 0
tmp <- tmp + (pi_0 * gaussian_pdf(zstat[i], nullmean, nullvar))
tmp <- tmp / (1.0 - pi_1)
df0[i] <- tmp
}
pi_true <- pi_1
# Re-scale variances
# set variance rescale gradient: 0.99 -> 0.98 -> ... -> 0.01
# loss function: all f0 values should be under f within reasonable areas
mm <- 0
vv <- 0
mm <- mm + nullmean
vv <- vv + nullvar
z_left = mm - sqrt(vv)
z_right =  mm + sqrt(vv)
f0_temp <- NULL
for (i in 1:ntick) {
f0_temp[i]<- f0[i]
}
f_ord <- sort(f)
tol_diff <- f_ord[np] * 0.01
ct <- 0
ntry <- 10000
rescale_factor <- 1
is_under <- 1
is_over <- 0
while(rescale_factor > 0 && ct < ntry) {
#if(ct > 0 && ct %% 1000 == 0) {
#    print('error')
#}
for(i in 1:ntick) {
tmp <- 0.0
tmp <- tmp + (pi_0 * gaussian_pdf(tickMarks[i], nullmean, (nullvar * rescale_factor)))
tmp <- tmp / (1.0 - pi_1)
f0[i] <-  tmp # equalizing the height as it was obtained before rescaling
}
# f0_factor is updated as we shrink the variances
i <- 1
while(tickMarks[i] < nullmean) {
i <- i + 1
}
f0_factor = f[i] / f0[i]
for (i in 1:ntick) {
f0[i] <- f0[i] * f0_factor
}
# check if f0_temp falls under the non-parametric estimate of f
is_under <- 1
for (i in 1:ntick) {
if(tickMarks[i] > z_left && tickMarks[i] < z_right) {
if(f0[i] > (f[i]+tol_diff)) {
is_under <- 0
}
}
}
if(is_under==1) {
nullvar <- nullvar * rescale_factor
print(paste('Found re-scaling factor: ', rescale_factor, sep = ''))
break
}
rescale_factor <- rescale_factor * 0.99
ct <- ct + 1
}
if(ct == ntry) {
print(paste('Unable to rescale variances. Last try: ', rescale_factor, sep = ''))
}
#If it satisfied in the first try, that means we can inflate the variances
if(ct == 0) {
print("Inflating variances")
ct <- 0
rescale_factor <- 1
while(rescale_factor > 0 && ct < ntry) {
#if(ct > 0 && ct %% 1000 == 0) {
#    print('error')
#}
for(i in 1:ntick) {
tmp <- 0.0
tmp <- tmp + (pi_0 * gaussian_pdf(tickMarks[i], nullmean, (nullvar * rescale_factor)))
tmp <- tmp / (1.0 - pi_1)
f0[i] <-  tmp # equalizing the height as it was obtained before rescaling
}
# f0_factor is updated as we shrink the variances
i <- 1
while(tickMarks[i] < nullmean) {
i <- i + 1
}
f0_factor = f[i] / f0[i]
for (i in 1:ntick) {
f0[i] <- f0[i] * f0_factor
}
# check if f0_temp falls under the non-parametric estimate of f
is_over <- 0
for (i in 1:ntick) {
if(tickMarks[i] > z_left && tickMarks[i] < z_right) {
if(f0[i] > (f[i]+tol_diff)) {
is_over <- 1
}
}
}
if(is_over==1) {
nullvar <- nullvar * rescale_factor
print(paste('Found re-scaling factor: ', rescale_factor, sep = ''))
break
}
rescale_factor <- rescale_factor * 1.05
ct <- ct + 1
}
}
for(i in 1:ntick) {
if(f[i] < f0[i]) {
f[i] = f0[i]
}
}
# re-evaluate f0
for(i in 1:ntick) {
tmp = 0.0
tmp <- tmp + (pi_0 * gaussian_pdf(tickMarks[i], nullmean, nullvar))
tmp <- tmp / (1.0 - pi_1)
f0[i] = tmp
}
#computeProportion()
#get conservative estimates of pi_true: just use the data within a range: (-5,5) for now
#identifying the range to fit null component to the overall density
tmp_r <- NULL
mm <- 0
vv <- 0
mm <- mm + nullmean
vv <- vv + nullvar
tmp <- 1
for(i in 1:ntick) {
if(abs(tickMarks[i] - mm) < 3.0 * sqrt(vv)) {
tmp_r = f[i] / f0[i]
if(tmp_r < tmp) {
tmp <- tmp_r
}
}
}
pi_true = 1 - tmp
print(paste('The estimate of pi(DE) is: ', pi_true, sep = ''))
# Evaluate densities
for(j in 1:np) {
## Evaluate f0
df0[j] <- pi_0 * gaussian_pdf(zstat[j], nullmean, nullvar)
df0[j] <- df0[j] / (1.0 - pi_1)
## Evaluate df
i = 1
while(tickMarks[i] < zstat[j]  && i <= ntick) {
i <- i + 1
}
df[j] = f[i] + (f[i+1] - f[i]) / (tickMarks[i+1] - tickMarks[i]) * (zstat[j] - tickMarks[i])
}
## Evaluate f1
for(i in 1:ntick) {
f1[i] <- ( f[i] - (1.0-pi_true) * f0[i] ) / pi_true
if(f1[i] < 1e-100) {
f1[i] <- 1e-100
}
}
#computeFDR
QM_fdr <- NULL
QM_FDRup <- NULL
QM_FDRdown <- NULL
for(j in 1:np) {
QM_fdr[j] <- (1.0 - pi_true) * df0[j] / df[j]
if(QM_fdr[j] > 1) {
QM_fdr[j] <- 1
} else {
QM_fdr[j] <- QM_fdr[j]
}
}
example_qprot_fdr$QM_fdr <- QM_fdr
for(j in 1:np) {
i <- ntick
tmpsum <- 0.0
tmpsum0 <- 0.0
while(tickMarks[i-1] < zstat[j]) {
tmpsum <- tmpsum + (0.5 * (f[i] + f[i-1]) * (tickMarks[i] - tickMarks[i-1]))
tmpsum0 <- tmpsum0 + (0.5 * (f0[i] + f0[i-1]) * (tickMarks[i] - tickMarks[i-1]))
i <- i - 1
}
tmpsum <- tmpsum + (0.5 * (f[i] + df[j]) * (tickMarks[i] - zstat[j]))
tmpsum0 <- tmpsum0 + (0.5 * (f0[i] + df0[j]) * (tickMarks[i] - zstat[j]))
QM_FDRup[j] = (1.0 - pi_true) * tmpsum0 / tmpsum
if(QM_FDRup[j] > 1) {
QM_FDRup[j] <- 1
} else {
QM_FDRup[j] <- QM_FDRup[j]
}
}
example_qprot_fdr$QM_FDRup <- QM_FDRup
for(j in 1:np) {
i <- 1
tmpsum <- 0.0
tmpsum0 <- 0.0
while(tickMarks[i+1] < zstat[j]) {
tmpsum <- tmpsum + (0.5 * (f[i+1] + f[i]) * (tickMarks[i+1] - tickMarks[i]))
tmpsum0 <- tmpsum0 + (0.5 * (f0[i+1] + f0[i]) * (tickMarks[i+1] - tickMarks[i]))
i <- i + 1
}
tmpsum <- tmpsum + (0.5 * (df[j] + f[i]) * (zstat[j]  - tickMarks[i]))
tmpsum0 <- tmpsum0 + (0.5 * (df0[j] + f0[i]) * (zstat[j]  - tickMarks[i]))
QM_FDRdown[j] = (1.0 - pi_true) * tmpsum0 / tmpsum
if(QM_FDRdown[j] > 1) {
QM_FDRdown[j] <- 1
} else {
QM_FDRdown[j] <- QM_FDRdown[j]
}
}
example_qprot_fdr$QM_FDRdown <- QM_FDRdown
write.csv(example_qprot_fdr, 'QPROT_FDRcomp.csv')
View(zz)
library(clusterProfiler)
#library(ReactomePA)
#library(data.table)
#library(stringr)
library(tidyr)
#library(organism, character.only = TRUE)
library(tidyr)
# SET THE DESIRED ORGANISM HERE
organism = "org.Hs.eg.db"
#BiocManager::install(organism, character.only = TRUE)
library(organism, character.only = TRUE)
fileIn <- "QPROT_FDRcomp.csv"
cutOffs <- c(0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009,
0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1)
# Order by pValue
qprot_out <- qprot_out[order(qprot_out$fdr), ]
## Seperate the protein name so the Unipropt accesion can be used
protAcc <- qprot_out %>% separate(Protein, c("SP", "Uniprot_Acc", "Entrez"), sep = "\\|")
## List of background proteins for enrichment analysis
BG_prots <- protAcc$Uniprot_Acc
# Reads in file and removes top 2 lines
qprot_out <- read.csv(fileIn, header = TRUE)
qprot_out <- qprot_out[,c(1:19)]
View(qprot_out)
# Reads in file and removes top 2 lines
qprot_out <- read.csv(fileIn, header = TRUE)
qprot_out <- qprot_out[,c(2:20)]
cutOffs <- c(0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009,
0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1)
# Order by pValue
qprot_out <- qprot_out[order(qprot_out$fdr), ]
## Seperate the protein name so the Unipropt accesion can be used
protAcc <- qprot_out %>% separate(Protein, c("SP", "Uniprot_Acc", "Entrez"), sep = "\\|")
## List of background proteins for enrichment analysis
BG_prots <- protAcc$Uniprot_Acc
BG_react <- select(org.Hs.eg.db, BG_prots, "ENTREZID", "UNIPROT")
## create dataframe to store number of significant terms for each threshold
thresholdResults <- data.frame(matrix(ncol = 7, nrow = 0))
headers <- c("Threshold", "Go_BP", "Go_MF", "Go_CC",
"Kegg_Path", "Total", 'DEs')
colnames(thresholdResults) <- headers
print(paste("Processing file: ", fileIn, sep = ""))
print(Sys.time())
flush.console()
## Perform enrichment analysis for specified pValue cutoffs
for(c in cutOffs) {
fdrThresh <- c
#fdrThresh <- 0.05
print(paste("Processing threshold: ", c, sep = ""))
flush.console()
## Initialises vector of results
results <- NULL
# Create list of DE proteins and all proteins
DE_prots <- as.vector(subset(protAcc$Uniprot_Acc, protAcc$fdr < fdrThresh))
DE <- length(DE_prots)
NonDE_prots <- as.vector(subset(protAcc$Uniprot_Acc, protAcc$fdr >= fdrThresh))
nonDE <- length(NonDE_prots)
if (DE > 0) {
print(paste("Number of DEs: ", DE, sep = ""))
flush.console()
print("Performing GO analysis....")
flush.console()
# Get functional annotation chart as R object and save to file
GO_BPresult <- enrichGO(gene = DE_prots,
OrgDb = 'org.Hs.eg.db',
keyType = 'UNIPROT',
ont = 'BP',
universe = BG_prots,
pvalueCutoff = 0.05,
pAdjustMethod = 'BH')
GO_BPsimply <- simplify(GO_BPresult,
cutoff = 0.7,
by = "p.adjust",
select_fun = min)
GOBPnum <- nrow(GO_BPsimply)
GO_MFresult <- enrichGO(gene = DE_prots,
OrgDb = 'org.Hs.eg.db',
keyType = 'UNIPROT',
ont = 'MF',
universe = BG_prots,
pvalueCutoff = 0.05,
pAdjustMethod = 'BH')
GO_MFsimply <- simplify(GO_MFresult,
cutoff = 0.7,
by = "p.adjust",
select_fun = min)
GOMFnum <- nrow(GO_MFsimply)
GO_CCresult <- enrichGO(gene = DE_prots,
OrgDb = 'org.Hs.eg.db',
keyType = 'UNIPROT',
ont = 'CC',
universe = BG_prots,
pvalueCutoff = 0.05,
pAdjustMethod = 'BH')
GO_CCsimply <- simplify(GO_CCresult,
cutoff = 0.7,
by = "p.adjust",
select_fun = min)
GOCCnum <- nrow(GO_CCsimply)
totGO <- GOBPnum + GOMFnum + GOCCnum
print(paste("Number of GO terms: ", totGO, sep = ""))
flush.console()
print("Performing KEGG analysis")
flush.console()
KEGGresult <- enrichKEGG(gene = DE_prots,
organism = "hsa",
keyType = 'uniprot',
universe = BG_prots,
pvalueCutoff = 0.05,
pAdjustMethod = 'BH')
KEGGnum <- nrow(KEGGresult)
print(paste("Number of KEGG terms: ", KEGGnum , sep = ""))
flush.console()
totNum <- totGO + KEGGnum
## Summary of results
results <- data.frame(Threshold = fdrThresh, Go_BP = GOBPnum, Go_MF = GOMFnum,
Go_CC = GOCCnum,  Kegg_Path = KEGGnum , Total = totNum, DEs = DE)
## Add to table of results
thresholdResults <- rbind(thresholdResults, results)
} else {
results <- data.frame(Threshold =fdrThresh, Go_BP = 0, Go_MF = 0,
Go_CC = 0, Kegg_Path = 0, Total = 0, DEs = DE)
thresholdResults <- rbind(thresholdResults, results)
}
}
